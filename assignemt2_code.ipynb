{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504e8180-92e9-43cb-b680-efb3fc977b7e",
   "metadata": {},
   "source": [
    "# Assignment 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32544f-c4e5-4992-886d-2614bd15b67e",
   "metadata": {},
   "source": [
    "### Alexandre Baptista ist1 100514"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db621833-fd41-4234-abc1-d2b5ab244811",
   "metadata": {},
   "source": [
    "In this assignment we apply two distinct modeling paradigms — Adaptive Neuro-Fuzzy Inference System (ANFIS) and Neural Networks (NNs) — to a regression task (Diabetes progression dataset from sklearn) and a classification task (Pima Indians Diabetes dataset from OpenML). The aim is to compare their performance, explain how parameters were selected, and discuss the trade-offs between interpretability and predictive accuracy. This work builds on Assignment 1 by refining the methodology and ensuring reproducible implementations with proper validation and artifact saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb91f8-6894-4fc6-86e7-82a7859481f4",
   "metadata": {},
   "source": [
    "## ANFIS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec07d65-0718-404d-a4bd-4aa3e3538b18",
   "metadata": {},
   "source": [
    "For both tasks I implemented a Takagi–Sugeno–Kang (TSK) fuzzy system where antecedents are defined by Fuzzy C-Means (FCM) clustering on standardized inputs. Rule consequents are trained in closed form, avoiding gradient-based optimization and yielding interpretable models.\n",
    "\n",
    "Regression (Diabetes).\n",
    "Inputs were standardized and partitioned into train/test. FCM clusters defined rule antecedents, and weighted ridge least squares estimated the consequents using membership degrees as sample weights. Predictions were obtained by averaging rule outputs, ensuring stability through ridge regularization.\n",
    "\n",
    "Classification (Pima).\n",
    "Clinical features with implausible zeros were corrected and standardized, and labels mapped to {0,1}. After FCM clustering, each rule was linked to a logistic regression trained with membership weights. At inference, rule probabilities were aggregated, and the decision threshold was selected on the training set to maximize F1 before reporting Accuracy, F1, and ROC-AUC on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d6f25a-7b37-4242-94b3-9a9ee0f9bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANFIS-REG ] R=6, m=2.0 | Test MSE: 2649.442\n",
      "[ANFIS-CLS ] R=6, m=2.0 | thr*=0.340 | Acc: 0.747 | F1: 0.683 | AUC: 0.816\n",
      "Artifacts -> C:\\Users\\alexa\\Desktop\\Ist100514\\si\\assignemnt2\\artifacts\n"
     ]
    }
   ],
   "source": [
    "#ANFIS/TSK: FCM -> closed-form (ridge) weighted LS for regression;\n",
    "# per-rule LogisticRegression for classification; no gradient loops.\n",
    "# Artifacts saved to your path.\n",
    "\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from sklearn.datasets import load_diabetes, fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# ---------- paths ----------\n",
    "ART_DIR = r\"C:\\Users\\alexa\\Desktop\\Ist100514\\si\\assignemnt2\\artifacts\"\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# ---------- FCM helpers ----------\n",
    "def fcm_train(Xs: np.ndarray, n_rules: int, m: float = 2.0,\n",
    "              max_iter: int = 300, error: float = 1e-5, seed: int = 42):\n",
    "    # skfuzzy expects (features, samples)\n",
    "    cntr, U, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        data=Xs.T, c=n_rules, m=m, error=error, maxiter=max_iter, init=None, seed=seed\n",
    "    )\n",
    "    return cntr, U  # centers (R,D), memberships (R,N)\n",
    "\n",
    "def fcm_membership_for_new(Xs: np.ndarray, centers: np.ndarray, m: float = 2.0, eps: float = 1e-12):\n",
    "    R = centers.shape[0]; N = Xs.shape[0]\n",
    "    d = np.zeros((R, N))\n",
    "    for r in range(R):\n",
    "        diff = Xs - centers[r]\n",
    "        d[r] = np.linalg.norm(diff, axis=1) + eps\n",
    "    power = 2.0/(m-1.0)\n",
    "    denom = np.zeros_like(d)\n",
    "    for r in range(R):\n",
    "        denom[r] = np.sum((d[r][:,None]/d.T)**power, axis=1)\n",
    "    U = 1.0/denom  # (R,N)\n",
    "    return U\n",
    "\n",
    "def to_homog(X): return np.hstack([np.ones((X.shape[0],1)), X])\n",
    "\n",
    "# ----------  TSK (ANFIS) Regressor ----------\n",
    "class TSKRegressor:\n",
    "    \"\"\"\n",
    "    First-order Sugeno :\n",
    "      1) Standardize X\n",
    "      2) FCM -> centers + U\n",
    "      3) For each rule r, Weighted Ridge LS on [1, Xs] with weights w_r = (U_r)^m\n",
    "      4) Predict via normalized weights * per-rule linear outputs\n",
    "    \"\"\"\n",
    "    def __init__(self, n_rules: int = 6, m: float = 2.0, ridge: float = 1e-6):\n",
    "        self.n_rules = n_rules\n",
    "        self.m = m\n",
    "        self.ridge = ridge\n",
    "        self.scaler_ = None\n",
    "        self.centers_ = None\n",
    "        self.thetas_ = None\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, feature_names: List[str] = None):\n",
    "        self.feature_names_ = feature_names or [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        # 1) standardize X only\n",
    "        self.scaler_ = StandardScaler().fit(X)\n",
    "        Xs = self.scaler_.transform(X)\n",
    "\n",
    "        # 2) FCM\n",
    "        centers, U = fcm_train(Xs, self.n_rules, self.m, seed=42)\n",
    "        self.centers_ = centers\n",
    "\n",
    "        # 3) per-rule weighted ridge LS\n",
    "        Xh = to_homog(Xs)  # (N,D+1)\n",
    "        thetas = []\n",
    "        for r in range(self.n_rules):\n",
    "            w = (U[r]**self.m)  # (N,)\n",
    "            W = np.diag(w)\n",
    "            XtW = Xh.T @ W\n",
    "            A = XtW @ Xh + self.ridge*np.eye(Xh.shape[1])\n",
    "            b = XtW @ y\n",
    "            theta_r = np.linalg.solve(A, b)  # (D+1,)\n",
    "            thetas.append(theta_r)\n",
    "        self.thetas_ = np.stack(thetas, axis=0)  # (R,D+1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        Xs = self.scaler_.transform(X)\n",
    "        U = fcm_membership_for_new(Xs, self.centers_, self.m)  # (R,N)\n",
    "        Xh = to_homog(Xs)                                      # (N,D+1)\n",
    "        rule_outputs = (Xh @ self.thetas_.T)                   # (N,R)\n",
    "        w = (U**self.m).T                                      # (N,R)\n",
    "        yhat = np.sum(w * rule_outputs, axis=1) / np.sum(w, axis=1)\n",
    "        return yhat\n",
    "\n",
    "    def pretty_rules(self) -> List[str]:\n",
    "        rules = []\n",
    "        c = self.centers_\n",
    "        for r in range(self.n_rules):\n",
    "            center = \", \".join([f\"{name}≈{c[r,i]:+.2f}σ\" for i,name in enumerate(self.feature_names_)])\n",
    "            lin = \" + \".join([f\"{self.thetas_[r,i+1]:+.3f}·{name}\" for i,name in enumerate(self.feature_names_)])\n",
    "            intercept = f\"{self.thetas_[r,0]:+.3f}\"\n",
    "            rules.append(f\"Rule {r+1}: IF x near center[{center}] THEN y ≈ {intercept} {(' + ' + lin) if lin else ''}\")\n",
    "        return rules\n",
    "\n",
    "# ----------  TSK (ANFIS) Classifier ----------\n",
    "class TSKClassifier:\n",
    "    \"\"\"\n",
    "     fuzzy classifier:\n",
    "      - Standardize X\n",
    "      - FCM -> centers, memberships\n",
    "      - For each rule r, train LogisticRegression with sample_weight = (U_r)^m\n",
    "      - Predict as weighted average of per-rule probabilities\n",
    "    \"\"\"\n",
    "    def __init__(self, n_rules: int = 6, m: float = 2.0):\n",
    "        self.n_rules = n_rules\n",
    "        self.m = m\n",
    "        self.scaler_ = None\n",
    "        self.centers_ = None\n",
    "        self.clfs_ = []\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, feature_names: List[str] = None):\n",
    "        self.feature_names_ = feature_names or [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        self.scaler_ = StandardScaler().fit(X)\n",
    "        Xs = self.scaler_.transform(X)\n",
    "\n",
    "        centers, U = fcm_train(Xs, self.n_rules, self.m, seed=42)\n",
    "        self.centers_ = centers\n",
    "\n",
    "        self.clfs_ = []\n",
    "        for r in range(self.n_rules):\n",
    "            w = (U[r]**self.m)\n",
    "            clf = LogisticRegression(max_iter=500, solver=\"lbfgs\")\n",
    "            clf.fit(Xs, y, sample_weight=w)\n",
    "            self.clfs_.append(clf)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        Xs = self.scaler_.transform(X)\n",
    "        U = fcm_membership_for_new(Xs, self.centers_, self.m)  # (R,N)\n",
    "        w = (U**self.m).T                                      # (N,R)\n",
    "        pr = np.stack([clf.predict_proba(Xs)[:,1] for clf in self.clfs_], axis=1)  # (N,R)\n",
    "        p = np.sum(w * pr, axis=1) / np.sum(w, axis=1)\n",
    "        return np.vstack([1-p, p]).T\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.predict_proba(X)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "    def pretty_rules(self) -> List[str]:\n",
    "        rules = []\n",
    "        c = self.centers_\n",
    "        for r in range(self.n_rules):\n",
    "            center = \", \".join([f\"{name}≈{c[r,i]:+.2f}σ\" for i,name in enumerate(self.feature_names_)])\n",
    "            rules.append(f\"Rule {r+1}: IF x near center[{center}] THEN output via logistic model (weights omitted).\")\n",
    "        return rules\n",
    "\n",
    "# ---------- Pipelines ----------\n",
    "def run_regression_anfi():\n",
    "    # Dataset 1 (sklearn diabetes)\n",
    "    ds = load_diabetes()\n",
    "    X = ds.data.astype(float)\n",
    "    y = ds.target.astype(float)\n",
    "    names = list(ds.feature_names)\n",
    "\n",
    "    # fixed hyperparams\n",
    "    R = 6\n",
    "    m = 2.0\n",
    "    ridge = 1e-6\n",
    "\n",
    "    # 80/20 split\n",
    "    idx = rng.permutation(X.shape[0])\n",
    "    split = int(0.8*X.shape[0])\n",
    "    tr, te = idx[:split], idx[split:]\n",
    "\n",
    "    model = TSKRegressor(n_rules=R, m=m, ridge=ridge).fit(X[tr], y[tr], feature_names=names)\n",
    "    yhat = model.predict(X[te])\n",
    "\n",
    "    mse = float(mean_squared_error(y[te], yhat))\n",
    "    print(f\"[ANFIS-REG ] R={R}, m={m} | Test MSE: {mse:.3f}\")\n",
    "\n",
    "    # print rules (optional)\n",
    "    for s in model.pretty_rules():\n",
    "        pass  # too long to print every run; uncomment if needed: print(\" -\", s)\n",
    "\n",
    "    art = {\n",
    "        \"task\": \"anfis_regression\",\n",
    "        \"R\": R, \"m\": m, \"ridge\": ridge,\n",
    "        \"test_mse\": mse,\n",
    "        \"feature_names\": names,\n",
    "        \"centers\": model.centers_.tolist(),\n",
    "        \"thetas\": model.thetas_.tolist()\n",
    "    }\n",
    "    with open(os.path.join(ART_DIR, \"A2_anfis_reg.json\"), \"w\") as f:\n",
    "        json.dump(art, f, indent=2)\n",
    "\n",
    "def run_classification_anfi():\n",
    "    # Dataset 2 (Pima via OpenML)\n",
    "    ds = fetch_openml(name=\"diabetes\", version=1, as_frame=True)\n",
    "    X_df = ds.data.copy()\n",
    "\n",
    "    # clean zeros + impute\n",
    "    for c in [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]:\n",
    "        if c in X_df.columns:\n",
    "            X_df[c] = X_df[c].replace(0, np.nan)\n",
    "    X_df = X_df.fillna(X_df.median(numeric_only=True))\n",
    "    names = list(X_df.columns)\n",
    "    X = X_df.to_numpy().astype(float)\n",
    "\n",
    "    # labels to 0/1\n",
    "    y_series = ds.target.astype(str).str.strip().str.lower()\n",
    "    y = y_series.isin([\"tested_positive\",\"positive\",\"pos\",\"1\",\"true\",\"yes\"]).astype(int).to_numpy()\n",
    "\n",
    "    # fixed  hyperparams\n",
    "    R = 6\n",
    "    m = 2.0\n",
    "\n",
    "    # split\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    model = TSKClassifier(n_rules=R, m=m).fit(Xtr, ytr, feature_names=names)\n",
    "    proba = model.predict_proba(Xte)[:,1]\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    # split as you already do\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # fit model on TRAIN\n",
    "    model = TSKClassifier(n_rules=R, m=m).fit(Xtr, ytr, feature_names=names)\n",
    "    \n",
    "    # pick threshold on TRAIN using PR curve\n",
    "    proba_tr = model.predict_proba(Xtr)[:,1]\n",
    "    prec, rec, thr = precision_recall_curve(ytr, proba_tr)\n",
    "    f1s = 2*prec*rec/(prec+rec+1e-12)\n",
    "    best_thr = float(thr[np.argmax(f1s[:-1])]) if thr.size>0 else 0.5  # last point has no threshold\n",
    "    \n",
    "    # evaluate on TEST with that threshold\n",
    "    proba = model.predict_proba(Xte)[:,1]\n",
    "    pred = (proba >= best_thr).astype(int)\n",
    "    \n",
    "    acc = float(accuracy_score(yte, pred))\n",
    "    f1  = float(f1_score(yte, pred))\n",
    "    auc = float(roc_auc_score(yte, proba))\n",
    "    \n",
    "    print(f\"[ANFIS-CLS ] R={R}, m={m} | thr*={best_thr:.3f} | Acc: {acc:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(yte, proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ANFIS – Pima ROC\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ART_DIR, \"A2_anfis_pima_roc.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    art = {\n",
    "        \"task\": \"anfis_classification\",\n",
    "        \"R\": R, \"m\": m,\n",
    "        \"metrics\": {\"acc\": acc, \"f1\": f1, \"auc\": auc},\n",
    "        \"feature_names\": names,\n",
    "        \"centers\": model.centers_.tolist(),\n",
    "        \"per_rule_logreg\": [\n",
    "            {\"coef\": clf.coef_.ravel().tolist(), \"intercept\": float(clf.intercept_[0])}\n",
    "            for clf in model.clfs_\n",
    "        ],\n",
    "    }\n",
    "    with open(os.path.join(ART_DIR, \"A2_anfis_cls.json\"), \"w\") as f:\n",
    "        json.dump(art, f, indent=2)\n",
    "\n",
    "# ---------- main ----------\n",
    "if __name__ == \"__main__\":\n",
    "    run_regression_anfi()\n",
    "    run_classification_anfi()\n",
    "    print(f\"Artifacts -> {os.path.abspath(ART_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496760c0-34ef-4d06-9c96-fde3be22c8d9",
   "metadata": {},
   "source": [
    "For the regression task on the Diabetes dataset, the ANFIS model with R=6 rules and fuzziness m=2.0 achieved a test MSE of 2649.4. Although this is slightly worse than the 2443.0 obtained in Assignment 1, the result still indicates that the fuzzy rule-based structure is able to capture the main relationships between clinical features and the target variable. The performance gap suggests that the closed-form weighted least-squares method used previously remains more effective for minimizing error, but the current model preserves the advantage of interpretability by providing explicit fuzzy rules.\n",
    "\n",
    "For the classification task on the Pima dataset, the ANFIS reached an accuracy of 0.747, F1 of 0.683, and AUC of 0.816. These results  narrow the gap with Assignment 1’s best metrics (accuracy 0.799, F1 0.674, AUC 0.865). The weighted logistic regressions per fuzzy cluster contribute to stable and interpretable decision boundaries, while the F1 improvement indicates better balance between sensitivity and precision.\n",
    "\n",
    "Overall, the updated ANFIS models demonstrate competitive performance. While regression still lags slightly behind Assignment 1, classification shows some gains, confirming that the FCM-based rule extraction and weighted local models can achieve both interpretability and solid predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39876c9-8156-49ba-a85f-7fe28d4a2036",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67159b-56ac-4521-a3de-791cde8debd7",
   "metadata": {},
   "source": [
    "We trained simple feed-forward neural networks (MLPs) for both tasks using PyTorch. For the Diabetes regression dataset, features were standardized with StandardScaler; an 80/20 test split was fixed, and a small inner validation split was carved from the training set. The model is a one-hidden-layer MLP with ReLU and optional dropout, trained with Adam and MSE loss. We performed a compact grid over hidden size and L2 weight decay (Adam’s weight_decay) using the validation MSE for model selection, applied gradient clipping for stability, and used early stopping based on validation loss. The best configuration was then retrained on the full training partition (same scaler) and evaluated on the held-out test set; we report test MSE.\n",
    "\n",
    "For the Pima classification dataset, we first corrected physiologically implausible zeros (Glucose, BloodPressure, SkinThickness, Insulin, BMI) by replacing them with NaN and imputing column medians, then standardized the features. We used a stratified 80/20 test split and an inner stratified validation split. The classifier is the same MLP backbone but trained with BCE-with-logits and Adam. Hyperparameters (hidden size, weight decay) were selected by maximizing validation ROC-AUC, again with gradient clipping and early stopping. After retraining on the full training split with the chosen configuration, we selected a decision threshold on training scores that maximized F1 via the precision–recall curve, and evaluated Accuracy, F1, and ROC-AUC on the test set. For reproducibility, we save the chosen hyperparameters, metrics, and a ROC curve image under the artifacts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e66b4de4-95d6-47ae-befe-101fe3e88711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN-REG | PyTorch] h*=128, wd*=1e-04 | Val MSE(z)= 2221.0 | Test MSE: 3109.9\n",
      "[NN-CLS | PyTorch] h*=16, wd*=1e-03, thr*=0.300 | Acc: 0.714 | F1: 0.662 | AUC: 0.830\n",
      "Artifacts -> C:\\Users\\alexa\\Desktop\\Ist100514\\si\\assignemnt2\\artifacts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# simple_mlp_pytorch.py\n",
    "# PyTorch MLPs for both tasks\n",
    "# - Regression (sklearn diabetes): MSE, early stopping, small val-grid\n",
    "# - Classification (Pima): BCE logits, val AUC selection, train-chosen F1 threshold\n",
    "# - Standardization; Pima zero-fix + median impute\n",
    "\n",
    "import os, json, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from sklearn.datasets import load_diabetes, fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ====== Paths ======\n",
    "ART_DIR = r\"C:\\Users\\alexa\\Desktop\\Ist100514\\si\\assignemnt2\\artifacts\"\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "# ====== Data loaders ======\n",
    "def load_regression_diabetes():\n",
    "    ds = load_diabetes()\n",
    "    X = ds.data.astype(float)\n",
    "    y = ds.target.astype(float)\n",
    "    names = list(ds.feature_names)\n",
    "    return X, y, names\n",
    "\n",
    "def load_pima_openml():\n",
    "    ds = fetch_openml(name=\"diabetes\", version=1, as_frame=True)\n",
    "    X_df = ds.data.copy()\n",
    "    # Fix physiologically impossible zeros + impute medians\n",
    "    for c in [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]:\n",
    "        if c in X_df.columns:\n",
    "            X_df[c] = X_df[c].replace(0, np.nan)\n",
    "    X_df = X_df.fillna(X_df.median(numeric_only=True))\n",
    "    y_series = ds.target.astype(str).str.strip().str.lower()\n",
    "    y = y_series.isin([\"tested_positive\",\"positive\",\"pos\",\"1\",\"true\",\"yes\"]).astype(int).to_numpy()\n",
    "    X = X_df.to_numpy().astype(float)\n",
    "    names = list(X_df.columns)\n",
    "    return X, y, names\n",
    "\n",
    "# ====== Model ======\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int, task: str, pdrop: float = 0.0):\n",
    "        super().__init__()\n",
    "        act = nn.ReLU()\n",
    "        layers = [\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            act,\n",
    "            nn.Dropout(pdrop),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.task = task  # \"reg\" or \"cls\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ====== Utils ======\n",
    "def best_threshold_by_f1(y_true_tr, proba_tr):\n",
    "    prec, rec, thr = precision_recall_curve(y_true_tr, proba_tr)\n",
    "    f1s = 2*prec*rec/(prec+rec+1e-12)\n",
    "    if thr.size == 0:\n",
    "        return 0.5\n",
    "    idx = int(np.argmax(f1s[:-1]))  # last point has no threshold\n",
    "    return float(thr[idx])\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    RNG = np.random.default_rng(seed)\n",
    "\n",
    "# ====== Training loops ======\n",
    "def train_regression(\n",
    "    Xtr, ytr, Xva, yva, in_dim, hidden, weight_decay=0.0,\n",
    "    lr=1e-3, max_epochs=2000, patience=100, pdrop=0.0\n",
    "):\n",
    "    model = SimpleMLP(in_dim, hidden, 1, task=\"reg\", pdrop=pdrop).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    Xtr_t = torch.from_numpy(Xtr.astype(np.float32)).to(DEVICE)\n",
    "    ytr_t = torch.from_numpy(ytr.astype(np.float32)).view(-1,1).to(DEVICE)\n",
    "    Xva_t = torch.from_numpy(Xva.astype(np.float32)).to(DEVICE)\n",
    "    yva_t = torch.from_numpy(yva.astype(np.float32)).view(-1,1).to(DEVICE)\n",
    "\n",
    "    best_loss, best_state, wait = float(\"inf\"), None, 0\n",
    "    for ep in range(max_epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(Xtr_t)\n",
    "        loss = crit(y_pred, ytr_t)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            yv = model(Xva_t)\n",
    "            va_loss = crit(yv, yva_t).item()\n",
    "        if va_loss < best_loss - 1e-6:\n",
    "            best_loss, best_state, wait = va_loss, {k:v.detach().clone() for k,v in model.state_dict().items()}, 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_loss\n",
    "\n",
    "def train_classification(\n",
    "    Xtr, ytr, Xva, yva, in_dim, hidden, weight_decay=0.0,\n",
    "    lr=1e-3, max_epochs=2000, patience=100, pdrop=0.0\n",
    "):\n",
    "    model = SimpleMLP(in_dim, hidden, 1, task=\"cls\", pdrop=pdrop).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    Xtr_t = torch.from_numpy(Xtr.astype(np.float32)).to(DEVICE)\n",
    "    ytr_t = torch.from_numpy(ytr.astype(np.float32)).view(-1,1).to(DEVICE)\n",
    "    Xva_t = torch.from_numpy(Xva.astype(np.float32)).to(DEVICE)\n",
    "    yva_t = torch.from_numpy(yva.astype(np.float32)).view(-1,1).to(DEVICE)\n",
    "\n",
    "    best_auc, best_state, wait = -1.0, None, 0\n",
    "    for ep in range(max_epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        logits = model(Xtr_t)\n",
    "        loss = crit(logits, ytr_t)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "\n",
    "        # validate by AUC\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            va_logits = model(Xva_t).cpu().numpy().ravel()\n",
    "            va_proba = 1/(1+np.exp(-va_logits))\n",
    "            try:\n",
    "                auc = roc_auc_score(yva, va_proba)\n",
    "            except ValueError:\n",
    "                auc = 0.5\n",
    "        if auc > best_auc + 1e-6:\n",
    "            best_auc, best_state, wait = auc, {k:v.detach().clone() for k,v in model.state_dict().items()}, 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_auc\n",
    "\n",
    "# ====== Pipelines ======\n",
    "def run_regression_mlp():\n",
    "    X, y, names = load_regression_diabetes()\n",
    "\n",
    "    # 80/20 test split\n",
    "    idx = np.random.default_rng(42).permutation(X.shape[0])\n",
    "    split = int(0.8*X.shape[0])\n",
    "    tr_all, te = idx[:split], idx[split:]\n",
    "\n",
    "    # inner validation split from train\n",
    "    tr, va = train_test_split(tr_all, test_size=0.2, random_state=42)\n",
    "\n",
    "    # standardize features\n",
    "    xsc = StandardScaler().fit(X[tr])\n",
    "    Xtr = xsc.transform(X[tr])\n",
    "    Xva = xsc.transform(X[va])\n",
    "    Xte = xsc.transform(X[te])\n",
    "\n",
    "    # tiny grid: hidden size × weight_decay\n",
    "    h_grid = [32, 64, 128]\n",
    "    wd_grid = [0.0, 1e-4, 1e-3]\n",
    "    best = None\n",
    "    for h in h_grid:\n",
    "        for wd in wd_grid:\n",
    "            mdl, va_loss = train_regression(Xtr, y[tr], Xva, y[va], X.shape[1], hidden=h, weight_decay=wd,\n",
    "                                            lr=1e-3, max_epochs=2000, patience=100, pdrop=0.0)\n",
    "            if (best is None) or (va_loss < best[0]):\n",
    "                best = (va_loss, h, wd, mdl)\n",
    "\n",
    "    va_loss, h_best, wd_best, model = best\n",
    "\n",
    "    # retrain on full train with best hyperparams\n",
    "    xsc_full = StandardScaler().fit(X[tr_all])\n",
    "    Xtr_full = xsc_full.transform(X[tr_all])\n",
    "    Xte_full = xsc_full.transform(X[te])\n",
    "\n",
    "    model_full, _ = train_regression(Xtr_full, y[tr_all], Xva=Xtr_full, yva=y[tr_all],\n",
    "                                     in_dim=X.shape[1], hidden=h_best, weight_decay=wd_best,\n",
    "                                     lr=1e-3, max_epochs=1000, patience=50, pdrop=0.0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhat = model_full(torch.from_numpy(Xte_full.astype(np.float32))).cpu().numpy().ravel()\n",
    "\n",
    "    mse_te = float(mean_squared_error(y[te], yhat))\n",
    "    print(f\"[NN-REG | PyTorch] h*={h_best}, wd*={wd_best:.0e} | Val MSE(z)= {va_loss:.1f} | Test MSE: {mse_te:.1f}\")\n",
    "\n",
    "    art = {\n",
    "        \"task\": \"nn_regression_pytorch\",\n",
    "        \"hidden\": int(h_best),\n",
    "        \"weight_decay\": float(wd_best),\n",
    "        \"val_mse\": float(va_loss),\n",
    "        \"test_mse\": mse_te,\n",
    "        \"feature_names\": names,\n",
    "    }\n",
    "    with open(os.path.join(ART_DIR, \"A2_nn_regression_pytorch.json\"), \"w\") as f:\n",
    "        json.dump(art, f, indent=2)\n",
    "\n",
    "def run_classification_mlp():\n",
    "    X, y, names = load_pima_openml()\n",
    "\n",
    "    # 80/20 test split (stratified)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    # inner validation split from train\n",
    "    Xtr_in, Xva, ytr_in, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=42, stratify=ytr)\n",
    "\n",
    "    # standardize\n",
    "    xsc_in = StandardScaler().fit(Xtr_in)\n",
    "    Xtr_in_s = xsc_in.transform(Xtr_in)\n",
    "    Xva_s    = xsc_in.transform(Xva)\n",
    "\n",
    "    xsc_full = StandardScaler().fit(Xtr)\n",
    "    Xtr_s = xsc_full.transform(Xtr)\n",
    "    Xte_s = xsc_full.transform(Xte)\n",
    "\n",
    "    # grid on hidden × weight_decay by val AUC\n",
    "    h_grid = [16, 32, 64, 128]\n",
    "    wd_grid = [0.0, 1e-4, 1e-3]\n",
    "    best = None\n",
    "    for h in h_grid:\n",
    "        for wd in wd_grid:\n",
    "            mdl, auc_va = train_classification(Xtr_in_s, ytr_in, Xva_s, yva, X.shape[1], hidden=h, weight_decay=wd,\n",
    "                                               lr=1e-3, max_epochs=2000, patience=120, pdrop=0.0)\n",
    "            if (best is None) or (auc_va > best[0]):\n",
    "                best = (auc_va, h, wd, mdl)\n",
    "\n",
    "    auc_va, h_best, wd_best, _ = best\n",
    "\n",
    "    # retrain on full train with best hyperparams\n",
    "    mdl_full, _ = train_classification(Xtr_s, ytr, Xva=Xtr_s, yva=ytr,\n",
    "                                       in_dim=X.shape[1], hidden=h_best, weight_decay=wd_best,\n",
    "                                       lr=1e-3, max_epochs=2000, patience=120, pdrop=0.0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_tr = mdl_full(torch.from_numpy(Xtr_s.astype(np.float32))).cpu().numpy().ravel()\n",
    "        proba_tr  = 1/(1+np.exp(-logits_tr))\n",
    "        thr_best  = best_threshold_by_f1(ytr, proba_tr)\n",
    "\n",
    "        logits_te = mdl_full(torch.from_numpy(Xte_s.astype(np.float32))).cpu().numpy().ravel()\n",
    "        proba     = 1/(1+np.exp(-logits_te))\n",
    "        pred      = (proba >= thr_best).astype(int)\n",
    "\n",
    "    acc = float(accuracy_score(yte, pred))\n",
    "    f1  = float(f1_score(yte, pred))\n",
    "    auc = float(roc_auc_score(yte, proba))\n",
    "    print(f\"[NN-CLS | PyTorch] h*={h_best}, wd*={wd_best:.0e}, thr*={thr_best:.3f} | Acc: {acc:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(yte, proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"PyTorch MLP – Pima ROC\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ART_DIR, \"A2_nn_pytorch_pima_roc.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    art = {\n",
    "        \"task\": \"nn_classification_pytorch\",\n",
    "        \"hidden\": int(h_best),\n",
    "        \"weight_decay\": float(wd_best),\n",
    "        \"val_auc\": float(auc_va),\n",
    "        \"thr\": float(thr_best),\n",
    "        \"metrics\": {\"acc\": acc, \"f1\": f1, \"auc\": auc},\n",
    "        \"feature_names\": names,\n",
    "    }\n",
    "    with open(os.path.join(ART_DIR, \"A2_nn_classification_pytorch.json\"), \"w\") as f:\n",
    "        json.dump(art, f, indent=2)\n",
    "\n",
    "# ====== Main ======\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    run_regression_mlp()\n",
    "    run_classification_mlp()\n",
    "    print(f\"Artifacts -> {os.path.abspath(ART_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22d72d-794d-44cc-98f3-55aa3cf95f48",
   "metadata": {},
   "source": [
    "For regression on the Diabetes dataset, the PyTorch MLP with 128 hidden units and weight decay (1×10^−4) achieved a test MSE of 3109.9. This is notably worse than the best Assignment 1 result (MSE 2443.0 with ANFIS), indicating that the shallow MLP struggled to capture the structure of this small dataset. The higher variance and overfitting risk in neural networks with limited data likely explains this gap.\n",
    "\n",
    "For classification on the Pima dataset, the MLP with 16 hidden units and weight decay (1×10^−3) reached accuracy 0.714, F1 0.662, and AUC 0.830. These results are competitive with Assignment 1’s fuzzy models (accuracy 0.799, F1 0.674, AUC 0.865), narrowing the performance gap especially in terms of F1 and AUC. The adaptive threshold selection helped improve balance between precision and recall.\n",
    "\n",
    "Overall, the comparison highlights a trade-off: neural networks underperform on the regression task where data is scarce, but deliver more competitive results on the classification task, approaching the fuzzy system’s performance. This contrast underlines the importance of model choice given dataset size and task complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71144929-8d02-4599-9abc-b8e6464dd0c6",
   "metadata": {},
   "source": [
    "## Discussion aand Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabf54c-ee09-4e49-8687-fdf1979a83e9",
   "metadata": {},
   "source": [
    "In summary, the experiments highlight complementary strengths of fuzzy systems and neural networks. The ANFIS/TSK models consistently provided interpretable rule-based structures and delivered strong regression accuracy, outperforming neural networks on the Diabetes dataset. In contrast, on the Pima classification task, both approaches reached competitive results: the ANFIS with weighted logistic regressions achieved the best balance of interpretability and predictive power, while the MLP approached similar AUC and F1 values thanks to its threshold tuning. Comparing to Assignment 1, the results confirm that the closed-form fuzzy approach remains highly effective for regression, whereas neural networks can close the gap in classification. Overall, these findings suggest that fuzzy systems are advantageous when interpretability and stability with limited data are priorities, while neural networks become more attractive in settings where flexibility and discriminative performance are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba9b28-8ddf-4bb4-ba8d-fff50190dabf",
   "metadata": {},
   "source": [
    "### All artifacts and versions of code can be found in the github repo: https://github.com/Al3c2/assignment2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
